---
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
***

> This report is just for assessment purposes

*by Fernando Cort√©s Tejada* | [linkedin](https://www.linkedin.com/in/fernando-cortes-tejada/) | [github](https://github.com/fcortes10)

# Purchase Card Transactions

We have a collection of purchase card transactions for the Birmingham City Council. This is a historical open source dataset and can be found in this [link](https://data.birmingham.gov.uk/dataset/purchase-card-transactions).

The aim of this analysis is to **discover profiles** or **unusual transactions**. In Data Science language this can be read as clustering and anomalies detection problems.

The card transactions data starts in April 2014 and ends in January 2018. We want to use the most recent complete year for the analysis so we chose the whole 2017 year. When looking at the raw data, December 2017 file has different type of data, so the file might be wrong. We switched to December 2016 to November 2017.

We chose to approach this problem with `R` instead of `python` because the most robust method for clustering when you have different data types, e.g. numerical, logical, categorical and ordinal, is Gower's distance and is not yet well implemented in a python package. The mathematical definition of this distance can be found [here](https://statisticaloddsandends.wordpress.com/2021/02/23/what-is-gowers-distance/).

***

## Index
1. [Data reading](#data-reading)  
2. [Data cleaning](#data-cleaning)  
3. [Data exploration](#data-exploration)
4. [Feature engineering](#feature-engineering)  
    + [Transaction level](#transaction-level)
    + [Client level](#client-level)  
5. [Clustering and anomalies detection](#clustering-and-anomalies-detection)  
    + [Number of clusters (K)](#number-of-clusters-k)  
    + [Clustering](#clustering)  
    + [Anomalies detection](#anomalies-detection)  
    + [New number of clusters (K)](#new-number-of-clusters-k)  
    + [New clustering](#new-clustering)  
6. [Interpretation and conclusions](#interpretation-and-conclusions)

***

## Let's get started

We begin by setting some global configurations and loading required packages. For installing the ones you don't have run `install.packages("package-name")` in the R console.

```{r config-packages, message=FALSE, cache=FALSE, results='hide'}
Sys.setlocale("LC_TIME", "C")

library(knitr)
library(data.table)
library(plyr)
library(readr)
library(stringr)
library(dplyr)
library(ggQC)
library(fastDummies)
library(cluster)
library(factoextra)
library(purrr)
library(mclust)
library(xgboost)
```

([back to index](#index))

***
### Data reading  

Then we read the data structure, get all files from the `data-csv` folder, also [here](https://github.com/fcortes10/purchases-clustering/tree/main/data-csv), standardize column names and append all datasets into a big one called `dt`.

```{r data reading, message=FALSE, cache=FALSE, results='hide'}
#reading structure from data
dt <- fread('data-csv/purchase-card-transactions-201612.csv', nrows = 0)

#getting file names
files <- list.files('data-csv', pattern = '..csv', full.names = TRUE)

#standardizing column names: no spaces and uppercase
column.names <- toupper(gsub(" ", "_", colnames(dt)))

#read all files and store them in a list
list.dt <- lapply(files, fread, select = colnames(dt), col.names = column.names)

#collapse all elements in one data.table
dt <- as.data.table(ldply(list.dt, data.frame))

#remove unnecessary/remaining environment variables and cleaning garbage in RAM
rm(list.dt, files, column.names)
gc()
```

Now we can have an overview of how the dataset looks like (just the 3 first rows)

```{r data_head, cache=FALSE}
head(dt, 3)
```

([back to index](#index))

***
### Data cleaning

We apply some treatments to data in order to make it more handleable. This includes, converting data types, changing formats, dropping columns, cleaning missing values, etc.

```{r data_cleaning_1}
#data cleaning
#extract just the numeric part from the card number as key (leaving it as string)
dt[ , CARD_NUMBER := str_pad(parse_number(dt[ , CARD_NUMBER]), width = 4, side = 'left', pad = "0")]

#transform the transaction date from string to date format
dt[ , TRANS_DATE := as.Date(dt[ , TRANS_DATE], format = "%d/%m/%y")]

#dropping transaction codes because we are keeping the description
dt[ , c("TRANS_CAC_CODE_1", "TRANS_CAC_CODE_2", "TRANS_CAC_CODE_3") := NULL]

#dropping TRANS_VAT_DESC because there is not metadata and we cannot infer its meaning
dt[ , TRANS_VAT_DESC := NULL]
```

We get a brief summary of the data to see any pattern or issue

```{r summary}
summary(dt)
```

The first variable we can see is `TRANS_DATE`, which has only one `NA`, so we remove it.

```{r drop_na}
dt <- dt[!is.na(TRANS_DATE)]
```

We also see that `ORIGINAL_GROSS_AMT` is a character column when it must be numeric. We cast it as numeric.

```{r cast_numeric_warning}
head(as.numeric(dt[ , ORIGINAL_GROSS_AMT]))
```

where we get a warning of induced `NAs`, so something must be happening. Checking the `NAs`

```{r na_check}
head(dt[which(is.na(as.numeric(dt[ , ORIGINAL_GROSS_AMT])))], 3)
```

we can see it is the thousands separator. So we replace the character `","` in the string and cast again.

```{r cast_numeric}
#it is the thousands separator, we replace it and cast again as numeric
dt[ , ORIGINAL_GROSS_AMT := as.numeric(gsub(",", "", ORIGINAL_GROSS_AMT))]
```

([back to index](#index))

***
### Data exploration

For the data exploration we don't want pretty charts yet, just see how the data looks. We start with out only numeric column `ORIGINAL_GROSS_AMT`. To explore a univariate numeric variable, the simplest way is plotting a histogram:

```{r histogram}
hist(dt[ , ORIGINAL_GROSS_AMT], main = "Histogram for gross amount", xlab = "Gross amount")
```

where we see that we got outliers that doesn't let us see clearly our data. So we limit our graph to be between the quantiles 5% and 95%
```{r quantiles_hist}
ext_q <- quantile(dt[ , ORIGINAL_GROSS_AMT], probs = c(0.05, 0.95))
hist(dt[between(ORIGINAL_GROSS_AMT, ext_q[1], ext_q[2]), ORIGINAL_GROSS_AMT], 
     main = "Histogram for gross amount (without tail values)", xlab = "Gross amount")
```

We can see a right skewed distribution, similar to a decaying exponential.

Now we will explore the categorical columns by checking the number of distinct values in each variable.

```{r unique_col_values}
#we declare a function for unique values
f <- function(x){
  length(unique(x))
}

#we apply the function to the margin 2 (columns)
apply(dt, MARGIN = 2, f)
```

We see that we have:  

* `370` distinct days with transactions
* `18,083` distinct monetary amounts in transactions
* `6,268` distinct merchant
* `1,028` distinct card numbers (and we can assume `1,028` distinct clients)
* `125` distinct type of business according to `DESC_1`
* `888` distinct type of business according to `DESC_2`
* `13` distinct type of business according to `DIRECTORATE`

We start with `TRANS_CAC_DESC_2` and show the 20 most frequent categories:

```{r desc_2}
head(dt[ , .N, TRANS_CAC_DESC_2][order(N, decreasing = TRUE)], 20)
```

where we can see that it is somehow related to institutions or schools but since we got no metadata and the are 88 categories we chose to drop it.

```{r drop_desc_2}
dt[ , TRANS_CAC_DESC_2 := NULL]
```

Then, we continue with the 20 most frequent `MERCHANT_NAME`

```{r merchant_name}
head(dt[ , .N, MERCHANT_NAME][order(N, decreasing = TRUE)], 20)
```

where we see that amazon has more than 5 variations in its name, so we group it

```{r amazon}
dt[grepl('amazon', tolower(dt[ , MERCHANT_NAME])), MERCHANT_NAME := 'amazon']
```

and we do the same for other similar cases.

```{r grouping_merchants}
dt[grepl('asda', tolower(dt[ , MERCHANT_NAME])), MERCHANT_NAME := 'asda']
dt[grepl('travelodge', tolower(dt[ , MERCHANT_NAME])), MERCHANT_NAME := 'travelodge']
dt[grepl('argos', tolower(dt[ , MERCHANT_NAME])), MERCHANT_NAME := 'argos']
```

We just keep the common merchants and everything else is grouped in a bag to reduce categories.

```{r merchants_bag}
head(dt[ , .N, MERCHANT_NAME][order(N, decreasing = TRUE)], 10)
common.merchants <- c('amazon', 'travelodge', 'asda', 'argos', 'post office counter')
dt[!dt[ , MERCHANT_NAME] %in% common.merchants, MERCHANT_NAME := 'other']
```

We do the same for `TRANS_CAC_DESC_1` and `DIRECTORATE` but we define a threshold of 5% not to put the category in a bag.

```{r casc_1}
#we show the 20 most frequent desc 1
head(dt[ , .N, TRANS_CAC_DESC_1][order(N, decreasing = TRUE)], 20)

#we keep the groups with more than 5% of total transactions and the rest is grouped in a bag
(gt5pct <- dt[ , .N, TRANS_CAC_DESC_1][order(N, decreasing = TRUE)][N > 0.05*nrow(dt), ][ , TRANS_CAC_DESC_1])
dt[!dt[ , TRANS_CAC_DESC_1] %in% gt5pct, TRANS_CAC_DESC_1 := 'other']
```

```{r directorate}
#we show the 10 most frequent directorate
dt[ , DIRECTORATE := toupper(DIRECTORATE)]
head(dt[ , .N, DIRECTORATE][order(N, decreasing = TRUE)], 10)

#we keep the groups with more than 5% of total transactions and the rest is grouped in a bag
(gt5pct <- dt[ , .N, DIRECTORATE][order(N, decreasing = TRUE)][N > 0.05*nrow(dt), ][ , DIRECTORATE])
dt[!dt[ , DIRECTORATE] %in% gt5pct, DIRECTORATE := 'other']
head(dt[ , .N, DIRECTORATE][order(N, decreasing = TRUE)], 10)
```

([back to index](#index))

***
### Feature engineering

We have divided the feature engineering in two groups: transaction level and client level.

#### Transaction level

We just have 6 columns and one is the key column (card_number) so just 5 features. Thus, we need to create more features in order to make clusters and find profiles.

Let's begin by extracting the day, weekday and month as variables.

```{r day_week_month}
#extract the day as a variable
dt[ , DAY := as.numeric(substr(x = TRANS_DATE, start = 9, stop = 10))]

#extract the weekday as a variable
dt[ , WEEKDAY := weekdays(dt[ , TRANS_DATE])]

#extract the months as a variable
dt[ , MONTH := as.numeric(substr(x = TRANS_DATE, start = 6, stop = 7))]
```

We create the `CHARGEBACK` feature, which tells us if the transaction amount is negative (a return) 

```{r chargeback}
#chargebacks
dt[ , CHARGEBACK := ifelse(ORIGINAL_GROSS_AMT < 0, 1, 0)]
```

and with that we change all amounts to positive.

```{r positive_amt}
#amounts to positive
dt[ , POSITIVE_AMT := ifelse(CHARGEBACK == 1, -1*ORIGINAL_GROSS_AMT, ORIGINAL_GROSS_AMT)]
```

We also create binary features that indicate us if the transaction amount is an outlier, an extreme value, is a tail value or is over the median.

```{r outlier_ext}
#outliers
iqr <- quantile(dt[ , POSITIVE_AMT], probs = c(0.25, 0.75))
dt[ , OUTLIER := ifelse(!between(POSITIVE_AMT, iqr[1]-1.5*QCrange(iqr), iqr[2]+1.5*QCrange(iqr)), 1, 0)]

#extreme values
dt[ , EXTREME_VALUE := ifelse(!between(POSITIVE_AMT, iqr[1]-3*QCrange(iqr), iqr[2]+3*QCrange(iqr)), 1, 0)]

#tail values
tails <- quantile(dt[ , POSITIVE_AMT], probs = c(0.025, 0.975))
dt[ , TAIL_VALUE := ifelse(!between(POSITIVE_AMT, tails[1], tails[2]), 1, 0)]

#over the median (otm)
median.value <- median(dt[ , POSITIVE_AMT])
dt[ , OTM := ifelse(POSITIVE_AMT > median.value, 1, 0)]
```

Now we create two more binary features related to transactions made around the payday or on weekends.

```{r payday_weekend}
#transactions around payday
paydays <- c(1, 2, 14, 15, 16, 28, 29, 30, 31)
dt[ , PAYDAY_TRX := ifelse(DAY %in% paydays, 1, 0)]

#transactions on weekends
weekend.days <- c('Saturday', 'Sunday')
dt[ , WEEKEND_TRX := ifelse(WEEKDAY %in% weekend.days, 1, 0)]
```

We see a summary with the new features.

```{r summary_trx}
summary(dt)
```

([back to index](#index))

#### Client level

Based on what we have just engineered at the transaction level, we begin to create features by grouping the information by client. 

First, we need to create dummies from the categorical variables.

```{r dummy_engineering}
#select columns to make dummies
cols.for.dummy <- c('MERCHANT_NAME', 'TRANS_CAC_DESC_1', 'DIRECTORATE', 'WEEKDAY')
dt.dummies <- dummy_cols(dt, select_columns = cols.for.dummy)
```


Then we start grouping by client and creating new features. These new features include averages, totals, sums, maximums, percentages, ratios and modes.

```{r group_eng}
#declare the statistical mode function (since R doesn't have one)
getmode <- function(x) {
  uniqv <- unique(x)
  uniqv[which.max(tabulate(match(x, uniqv)))]
}

#group by client
dt.grouped <- dt.dummies[ , .(NUM_TRX = .N, AVG_TRX = mean(POSITIVE_AMT), 
                MAX_TRX = max(POSITIVE_AMT), NUM_CHARGEBACKS = sum(CHARGEBACK),
                PCT_CHARGEBACKS = sum(CHARGEBACK)/.N, 
                AVG_AMT_CHARGEBACKS = mean(POSITIVE_AMT*CHARGEBACK),
                PCT_AMT_CHARGEBACKS = sum(POSITIVE_AMT*CHARGEBACK)/sum(POSITIVE_AMT),
                NUM_OUTLIER = sum(OUTLIER), PCT_OUTLIER = sum(OUTLIER)/.N,
                NUM_XTRM_VALUE = sum(EXTREME_VALUE), PCT_XTRM_VALUE = sum(EXTREME_VALUE)/.N,
                NUM_TAIL_VALUE = sum(TAIL_VALUE), PCT_TAIL_VALUE = sum(TAIL_VALUE)/.N,
                NUM_OTM = sum(OTM), PCT_OTM = sum(OTM)/.N, 
                NUM_PAYDAY_TRX = sum(PAYDAY_TRX), PCT_PAYDAY_TRX = sum(PAYDAY_TRX)/.N,
                NUM_WEEKEND_TRX = sum(WEEKEND_TRX), PCT_WEEKEND_TRX = sum(WEEKEND_TRX)/.N,
                MODE_MERCHANT = as.factor(getmode(MERCHANT_NAME)), 
                MODE_CAC_1 = as.factor(getmode(TRANS_CAC_DESC_1)),
                MODE_DIRECT = as.factor(getmode(DIRECTORATE)), 
                MODE_DAY = as.factor(getmode(DAY)), 
                MODE_MONTH = as.factor(getmode(MONTH))), 
                CARD_NUMBER]


```

Finally, we see a summary of the new client-level features.

```{r summary_group}
summary(dt.grouped)
```

([back to index](#index))

***
### Clustering and anomalies detection

Now that we have a lot more features after the feature engineering step, we begin with out cluster analysis.

#### Number of clusters (K)

First, we have to determine the optimal number of clusters.

Since we have categorical and numerical features, we decided to use Gower's distance approach for the distance/dissimilarity matrix.

We begin by storing the key column in a vector and dropping it from the final dataset.

```{r key}
#we store the key
key <- dt.grouped[ , CARD_NUMBER]
dt.grouped[ , CARD_NUMBER := NULL]
```

Then we create the distance/dissimilarity matrix.

```{r gower_1}
#we create a distance/dissimilarity matrix
set.seed(100)
gower_dist <- daisy(as.data.frame(dt.grouped),
                    metric = "gower")
```

There are 3 common methods for determining the number of clusters:

* Elbow method  
* Silhouette score  
* Gap statistic method  

but the latter method can't be performed over a gower distance matrix, so we will decide with the Elbow and Silhouette methods.

We begin with the Elbow method.

```{r elbow}
#elbow method (within-clusters sum of squares)
wss <- function(k, x) {
  kmeans(x = x, k, nstart = 10)$tot.withinss
}

#number of clusters to be tested (from 1 to 7)
k <- 1:7
wss_val <- map_dbl(k, wss, gower_dist)

#plot
plot(k, wss_val, type = "b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K", ylab = "Total within-clusters sum of squares")

```

This is just a visual method, where the optimal number of clusters is determined by where the inflection points are located at. In this case we can see a first inflection point at 2 and then at 4, so those are going to be our options.

Then we proceed with the Silhouette method.

```{r silhouette}
#silhouette method 
silhouette_score <- function(k, x){
  km <- kmeans(x = x, centers = k, nstart = 10)
  ss <- silhouette(km$cluster, dist(x))
  mean(ss[ , 3])
}

#number of clusters to be tested (from 2 to 7)
k <- 2:7
avg_sil <- sapply(k, silhouette_score, gower_dist)

#plot
plot(k, avg_sil, type = 'b', pch = 19, frame = FALSE,
     xlab = 'Number of clusters K', ylab = 'Average Silhouette Scores')
```

In this method, the highest score determines the optimal value of the number of clusters. So here, we conclude that our first choice must be `K=2` and as a second choice `K=4`.

([back to index](#index))

#### Clustering

Then, we proceed to use `K=2` and use the K-means algorithm for clustering.

```{r kmeans_1}
set.seed(1)
#kmeans
km <- kmeans(x = gower_dist, centers = 2)

#cluster size
km$size

#cluster distribution
prop.table(km$size)
```

Here we see that we have unbalanced clusters, probably due to anomalies but we are not sure yet.

In order to know which variables are the most important in determining the cluster belonging of each client, we train a **supervised model** with cluster as the target. First we must convert categorical variables into dummies

```{r dummy_2}
cols.for.dummy.2 <- c('MODE_MERCHANT', 'MODE_CAC_1', 'MODE_DIRECT', 'MODE_DAY', 'MODE_MONTH')
dt.dummies.2 <- dummy_cols(dt.grouped, select_columns = cols.for.dummy.2, remove_selected_columns = TRUE)
```

and we bind the target column, which is the cluster (1 and 2) converted to 0 and 1 labels.

```{r dt_sup}
dt.sup <- cbind(dt.dummies.2, cluster = km$cluster-1)
```

Then we train a classification model, in this case it is a binary classification since we have 2 clusters but in order to make it for general purposes (more than 2 clusters) we train a multinomial xgboost model. We will not go in much depth on how to build a supervised model because is out of the scope of this report, but the comments in the code can work as hints.

```{r xgboost}
#number of classes
k <- length(km$size)
#definition of the multinomial model
xgb_params <- list("objective" = "multi:softprob",
                   "eval_metric" = "mlogloss",
                   "num_class" = k)
#number of max iterations
nround <- 500 
#number of folds for the cross validation
cv.nfold <- 5

#we declare the train data matrix
train_matrix <- xgb.DMatrix(data = as.matrix(dt.sup[ , -80]), label = dt.sup$cluster)

#we do a cross validation to get the best iteration parameter
set.seed(5)
cv_model <- xgb.cv(params = xgb_params,
                   data = train_matrix, 
                   nrounds = nround,
                   nfold = cv.nfold,
                   verbose = TRUE,
                   prediction = TRUE,
                   early_stopping_rounds = 10)

#we train the xgboost with the best number of iterations
set.seed(10)
bst_model <- xgb.train(params = xgb_params,
                       data = train_matrix,
                       nrounds = cv_model$best_iteration)
```

We now check the most important features. Since it is a tree-based model, the frequency of appearance of the features is a good indicator of the importance. We select the top 10 and plot them.

```{r plot_imp}
#we plot the top 10 variables
importance <- xgb.importance(feature_names = colnames(train_matrix), model = bst_model)
xgb.plot.importance(importance_matrix = importance, top_n = 10, measure = 'Frequency')
```

We now get the mean of the most important features grouped by cluster in order to see the differences between their profiles.

```{r cluster_features}
#select the features
imp_features <- c(head(importance[order(Frequency, decreasing = TRUE)], 10)[ , Feature], 'cluster')

#we use these features to see what characterizes the clusters
round(dt.sup[ , ..imp_features][ , lapply(.SD, mean), cluster][order(cluster)], 2)
```

We can see a lot of difference in `PCT_CHARGEBACKS`, where the cluster 0 has more than 80% negative transactions. Also, in the `AVG_TRX`, where the ticket per transaction of cluster 0 is over 100 times the ticket from cluster 1. The `PCT_OUTLIER` indicates us that more than 80% of the transaction amounts of cluster 0 qualified as outliers and `MODE_MONTH_5` tells us that in 76% of the clients in cluster 0, the month with most transactions was May, which is kind of wierd to be that concentrated in just one month. 

This latter feature must be an indicator of an anomaly so we will check it.

([back to index](#index))

#### Anomalies detection

We start checking `MODE_MONTH_5`.

First we get the card numbers from people that are on cluster 0 and have their transaction mode in May.

```{r card_numbers_may}
#card numbers from people that have their trx mode on may
index <- which((dt.sup[ , MODE_MONTH_5] == 1) & unname(km$cluster-1 == 0))
anomaly_card_number <- key[index]
```

Then we check what are all the transaction dates of those clients.

```{r trx_dates_anomaly}
#transaction dates of that clients
table(dt[CARD_NUMBER %in% anomaly_card_number, TRANS_DATE])
```

We can see that all their transactions are just placed on May and that it is their only transaction in the whole year.

There could be many reasons to explain this, for example it could be a fraud attack or maybe a promotion that happen to be on that exact date, but the only thing we are certain of is that it is an **anomaly**.

([back to index](#index))

#### New number of clusters (K)

Since this anomaly affect the clustering we were trying to make, we take them off and start over. It should be easy, just replicate the code from above but without the anomalies cluster.

We remove the observations from cluster 0.

```{r rm_clust_0}
dt.grouped.2 <- dt.grouped[km$cluster-1 != 0]
```

Then we create the distance/dissimilarity matrix.

```{r gower_2}
#we create a distance/dissimilarity matrix
set.seed(100)
gower_dist.2 <- daisy(as.data.frame(dt.grouped.2),
                      metric = "gower")
```

We proceed with the Elbow method.

```{r elbow_2}
#number of clusters to be tested (from 1 to 7)
k <- 1:7
wss_val <- map_dbl(k, wss, gower_dist.2)

#plot
plot(k, wss_val, type = "b", pch = 19, frame = FALSE, 
     xlab="Number of clusters K", ylab = "Total within-clusters sum of squares")

```

We can see now that the inflection point is at `K=3`.

Now we proceed with the Silhouette method.

```{r silhouette_2}
#number of clusters to be tested (from 2 to 7)
k <- 2:7
avg_sil <- sapply(k, silhouette_score, gower_dist.2)

#plot
plot(k, avg_sil, type = 'b', pch = 19, frame = FALSE,
     xlab = 'Number of clusters K', ylab = 'Average Silhouette Scores')
```

The higher value is at `K=2` and the second at `K=3`. But since we are combining this method with the Elbow method, our final choice will be now `K=3`. We must see that this option is equal to have tried `K=4` in the first iteration without dropping the anomalies and both method pointed as second high `K=4` before.

([back to index](#index))

#### New clustering

We proceed with `K=3`

```{r kmeans_2}
set.seed(1)
#kmeans
km <- kmeans(x = gower_dist.2, centers = 3)

#cluster size
km$size

#cluster distribution
prop.table(km$size)
```

Now the clusters are more balanced, probably indicating distinct identified profiles.

We train the supervised model to get the most important features for the cluster belonging of the clients.

```{r model_2}
cols.for.dummy.2 <- c('MODE_MERCHANT', 'MODE_CAC_1', 'MODE_DIRECT', 'MODE_DAY', 'MODE_MONTH')
dt.dummies.2 <- dummy_cols(dt.grouped.2, select_columns = cols.for.dummy.2, remove_selected_columns = TRUE)

#we bind the "target" column which is the cluster (1, 2 and 3) converted to 0, 1 and 2
dt.sup <- cbind(dt.dummies.2, cluster = km$cluster-1)

#number of classes
k <- length(km$size)
#definition of the multinomial model
xgb_params <- list("objective" = "multi:softprob",
                   "eval_metric" = "mlogloss",
                   "num_class" = k)
#number of max iterations
nround <- 500 
#number of folds for the cross validation
cv.nfold <- 5

#we declare the train data matrix
train_matrix <- xgb.DMatrix(data = as.matrix(dt.sup[ , -80]), label = dt.sup$cluster)

#we do a cross validation to get the best iteration parameter
set.seed(5)
cv_model <- xgb.cv(params = xgb_params,
                   data = train_matrix, 
                   nrounds = nround,
                   nfold = cv.nfold,
                   verbose = FALSE,
                   prediction = TRUE,
                   early_stopping_rounds = 10)

#we train the xgboost with the best number of iterations
set.seed(10)
bst_model <- xgb.train(params = xgb_params,
                       data = train_matrix,
                       nrounds = cv_model$best_iteration)
```

Now we plot the 15 most important variables.

```{r plot_imp_2}
#we plot the top 15 variables
importance <- xgb.importance(feature_names = colnames(train_matrix), model = bst_model)
xgb.plot.importance(importance_matrix = importance, top_n = 15, measure = 'Frequency')
```

We now get the mean of the most important features grouped by cluster in order to see the differences between their profiles.

```{r cluster_features_2}
#select the features
imp_features <- c(head(importance[order(Frequency, decreasing = TRUE)], 15)[ , Feature], 'cluster')

#we use these features to see what characterizes the clusters
round(dt.sup[ , ..imp_features][ , lapply(.SD, mean), cluster][order(cluster)], 2)
```

([back to index](#index))

***
### Interpretation and conclusions

We have 3 clusters and the means of the important features. We can now characterize those clusters into profiles to take any kind of further action or just to understand the types of population inside our data.

Let's begin with cluster 2:

> this cluster has steady transactions (almost no outliers) and 63% of its transactions are over the median but it also has the lowest avg ticket of the 3 groups, so it has steady over the median comsuptions without any high volume transaction. Also, it doesn't have a high amount of transactions and 77% of them are on vehicle fuel. No amount spent in education. This profile looks like a taxi driver, pizza delivery or low-middle income person that uses his car to work.

Then, for cluster 0:

> this cluster has the highest transaction ticket but not even 50% of them are over the median, so they must buy a lot (most number of transactions) of little stuff and sometimes big things due to the highest MAX_TRX. It has the most outliers and they are making transactions at least twice as much as cluster 1 and 4 times more than cluster 2 on the weekends, so they must be going on spending spree those days with high volume transactions. This profile looks like a wealthy person.

Finally, for cluster 1:

> this cluster has an average transaction ticket and an average number of transactions. They buy distinct varieties of things because of MODE_CAC_other and MODE_MERCHANT_other with high values. They don't use amazon and don't spend on fuel. More than 50% of their transactions are on school. This profile looks like a middle class student.

If you got this far, I hope you enjoyed it.

([back to index](#index))