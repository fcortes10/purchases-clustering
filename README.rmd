---
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
***

> This report is just for academical purposes

*by Fernando Cort√©s Tejada* | [linkedin](https://www.linkedin.com/in/fernando-cortes-tejada/) | [github](https://github.com/fcortes10)

# Purchase Card Transactions

We have a collection of purchase card transactions for the Birmingham City Council. This is a historical open source dataset and can be found in this [link](https://data.birmingham.gov.uk/dataset/purchase-card-transactions).

The aim of this analysis is to **discover profiles** or **unusual transactions**. In Data Science language this can be read as clustering and anomalies detection problems.

The card transactions data starts in April 2014 and ends in January 2018. We want to use the most recent complete year for the analysis so we chose the whole 2017 year. When looking at the raw data, December 2017 file has different type of data, so the file might be wrong. We switched to December 2016 to November 2017.

We chose to approach this problem with `R` instead of `python` because the most robust method for clustering when you have different data types, e.g. numerical, logical, categorical and ordinal, is Gower's distance and is not yet well implemented in a python package. The mathematical definition of this distance can be found [here](https://statisticaloddsandends.wordpress.com/2021/02/23/what-is-gowers-distance/).

## Let's get started

We begin by setting some global configurations and loading required packages. For installing the ones you don't have run `install.packages("package-name")` in the R console.

```{r config-packages, message=FALSE, cache=FALSE, results='hide'}
Sys.setlocale("LC_TIME", "C")

library(knitr)
library(data.table)
library(plyr)
library(readr)
library(stringr)
library(dplyr)
library(ggQC)
library(fastDummies)
library(cluster)
library(factoextra)
library(purrr)
library(mclust)
library(xgboost)
```

***
#### Data reading

Then we read the data structure, get all files from the `data-csv` folder, also [here](https://github.com/fcortes10/purchases-clustering/tree/main/data-csv), standardize column names and append all datasets into a big one called `dt`.

```{r data reading, message=FALSE, cache=FALSE, results='hide'}
#reading structure from data
dt <- fread('data-csv/purchase-card-transactions-201612.csv', nrows = 0)

#getting file names
files <- list.files('data-csv', pattern = '..csv', full.names = TRUE)

#standardizing column names: no spaces and uppercase
column.names <- toupper(gsub(" ", "_", colnames(dt)))

#read all files and store them in a list
list.dt <- lapply(files, fread, select = colnames(dt), col.names = column.names)

#collapse all elements in one data.table
dt <- as.data.table(ldply(list.dt, data.frame))

#remove unnecessary/remaining environment variables and cleaning garbage in RAM
rm(list.dt, files, column.names)
gc()
```

Now we can have an overview of how the dataset looks like (just the 3 first rows)

```{r data_head, cache=FALSE}
head(dt, 3)
```


***
#### Data cleaning

We apply some treatments to data in order to make it more handleable. This includes, converting data types, changing formats, dropping columns, cleaning missing values, etc.

```{r data_cleaning_1}
#data cleaning
#extract just the numeric part from the card number as key (leaving it as string)
dt[ , CARD_NUMBER := str_pad(parse_number(dt[ , CARD_NUMBER]), width = 4, side = 'left', pad = "0")]

#transform the transaction date from string to date format
dt[ , TRANS_DATE := as.Date(dt[ , TRANS_DATE], format = "%d/%m/%y")]

#dropping transaction codes because we are keeping the description
dt[ , c("TRANS_CAC_CODE_1", "TRANS_CAC_CODE_2", "TRANS_CAC_CODE_3") := NULL]

#dropping TRANS_VAT_DESC because there is not metadata and we cannot infer its meaning
dt[ , TRANS_VAT_DESC := NULL]
```

We get a brief summary of the data to see any pattern or issue

```{r summary}
summary(dt)
```

The first variable we can see is `TRANS_DATE`, which has only one `NA`, so we remove it.

```{r drop_na}
dt <- dt[!is.na(TRANS_DATE)]
```

We also see that `ORIGINAL_GROSS_AMT` is a character column when it must be numeric. We cast it as numeric.

```{r cast_numeric_warning}
head(as.numeric(dt[ , ORIGINAL_GROSS_AMT]))
```

where we get a warning of induced `NAs`, so something must be happening. Checking the `NAs`

```{r na_check}
head(dt[which(is.na(as.numeric(dt[ , ORIGINAL_GROSS_AMT])))], 3)
```

we can see it is the thousands separator. So we replace the character `","` in the string and cast again.

```{r cast_numeric}
#it is the thousands separator, we replace it and cast again as numeric
dt[ , ORIGINAL_GROSS_AMT := as.numeric(gsub(",", "", ORIGINAL_GROSS_AMT))]
```

***
#### Data exploration

For the data exploration we don't want pretty charts yet, just see how the data looks. We start with out only numeric column `ORIGINAL_GROSS_AMT`. To explore a univariate numeric variable, the simplest way is plotting a histogram:

```{r histogram}
hist(dt[ , ORIGINAL_GROSS_AMT])
```

where we see that we got outliers that doesn't let us see clearly our data. So we limit our graph to be between the quantiles 5% and 95%
```{r quantiles_hist}
ext_q <- quantile(dt[ , ORIGINAL_GROSS_AMT], probs = c(0.05, 0.95))
hist(dt[between(ORIGINAL_GROSS_AMT, ext_q[1], ext_q[2]), ORIGINAL_GROSS_AMT])
```

```{r}
#exploring number of distinct values in each column
#we declare a function for unique values
f <- function(x){
  length(unique(x))
}

#we apply the function to the margin 2 (columns)
apply(dt, MARGIN = 2, f)

#we see that 
#we have 370 different days with transactions
#18083 different monetary amounts in transactions
#6268 different merchants
#1028 different card numbers (assuming 1028 different clients)
#125 grouped type of business according to desc 1
#888 grouped type of business according to desc 2
#13 grouped type of business according to directorate

#we show the 20 most frequent desc 2
head(dt[ , .N, TRANS_CAC_DESC_2][order(N, decreasing = TRUE)], 20)

#we see that it is somehow related to institutions or schools but since we got no metadata and the are 88 categories we chose to drop it
dt[ , TRANS_CAC_DESC_2 := NULL]

#we show the 20 most frequent merchant names
head(dt[ , .N, MERCHANT_NAME][order(N, decreasing = TRUE)], 20)

#we see that amazon has more than 5 variations in its name, so we group it
dt[grepl('amazon', tolower(dt[ , MERCHANT_NAME])), MERCHANT_NAME := 'amazon']

#we do the same for other similar cases
dt[grepl('asda', tolower(dt[ , MERCHANT_NAME])), MERCHANT_NAME := 'asda']
dt[grepl('travelodge', tolower(dt[ , MERCHANT_NAME])), MERCHANT_NAME := 'travelodge']
dt[grepl('argos', tolower(dt[ , MERCHANT_NAME])), MERCHANT_NAME := 'argos']

#everything else below 500 trx is grouped in a bag
head(dt[ , .N, MERCHANT_NAME][order(N, decreasing = TRUE)], 20)
common.merchants <- c('amazon', 'travelodge', 'asda', 'argos', 'post office counter')
dt[!dt[ , MERCHANT_NAME] %in% common.merchants, MERCHANT_NAME := 'other']

#we show the 20 most frequent desc 1
head(dt[ , .N, TRANS_CAC_DESC_1][order(N, decreasing = TRUE)], 20)

#we keep the groups with more than 5% of total transactions and the rest is grouped in a bag
(gt5pct <- dt[ , .N, TRANS_CAC_DESC_1][order(N, decreasing = TRUE)][N > 0.05*nrow(dt), ][ , TRANS_CAC_DESC_1])
dt[!dt[ , TRANS_CAC_DESC_1] %in% gt5pct, TRANS_CAC_DESC_1 := 'other']

#we show the 10 most frequent directorate
dt[ , DIRECTORATE := toupper(DIRECTORATE)]
head(dt[ , .N, DIRECTORATE][order(N, decreasing = TRUE)], 10)

#we keep the groups with more than 5% of total transactions and the rest is grouped in a bag
(gt5pct <- dt[ , .N, DIRECTORATE][order(N, decreasing = TRUE)][N > 0.05*nrow(dt), ][ , DIRECTORATE])
dt[!dt[ , DIRECTORATE] %in% gt5pct, DIRECTORATE := 'other']
head(dt[ , .N, DIRECTORATE][order(N, decreasing = TRUE)], 10)
```

