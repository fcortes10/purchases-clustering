---
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
***

> This report is just for academical purposes

*by Fernando Cort√©s Tejada* | [linkedin](https://www.linkedin.com/in/fernando-cortes-tejada/) | [github](https://github.com/fcortes10)

# Purchase Card Transactions

We have a collection of purchase card transactions for the Birmingham City Council. This is a historical open source dataset and can be found in this [link](https://data.birmingham.gov.uk/dataset/purchase-card-transactions).

The aim of this analysis is to **discover profiles** or **unusual transactions**. In Data Science language this can be read as clustering and anomalies detection problems.

The card transactions data starts in April 2014 and ends in January 2018. We want to use the most recent complete year for the analysis so we chose the whole 2017 year. When looking at the raw data, December 2017 file has different type of data, so the file might be wrong. We switched to December 2016 to November 2017.

We chose to approach this problem with `R` instead of `python` because the most robust method for clustering when you have different data types, e.g. numerical, logical, categorical and ordinal, is Gower's distance and is not yet well implemented in a python package. The mathematical definition of this distance can be found [here](https://statisticaloddsandends.wordpress.com/2021/02/23/what-is-gowers-distance/).

## Let's get started

We begin by setting some global configurations and loading required packages. For installing the ones you don't have run `install.packages("package-name")` in the R console.

```{r config-packages, message=FALSE, cache=TRUE, results='hide'}
Sys.setlocale("LC_TIME", "C")

library(data.table)
library(plyr)
library(readr)
library(stringr)
library(dplyr)
library(ggQC)
library(fastDummies)
library(cluster)
library(factoextra)
library(purrr)
library(mclust)
library(xgboost)
```

***
#### Data reading

Then we read the data structure, get all files from the `data-csv` folder, also [here](https://github.com/fcortes10/purchases-clustering/tree/main/data-csv), standardize column names and append all datasets into a big one called `dt`.

```{r data reading, message=FALSE, cache=TRUE, results='hide'}
#reading structure from data
dt <- fread('data-csv/purchase-card-transactions-201612.csv', nrows = 0)

#getting file names
files <- list.files('data-csv', pattern = '..csv', full.names = TRUE)

#standardizing column names: no spaces and uppercase
column.names <- toupper(gsub(" ", "_", colnames(dt)))

#read all files and store them in a list
list.dt <- lapply(files, fread, select = colnames(dt), col.names = column.names)

#collapse all elements in one data.table
dt <- as.data.table(ldply(list.dt, data.frame))

#remove unnecessary/remaining environment variables and cleaning garbage in RAM
rm(list.dt, files, column.names)
gc()
```

***
#### Data cleaning

```{r data cleaning, cache=TRUE}
#data cleaning
#extract just the numeric part from the card number as key
dt[ , CARD_NUMBER := str_pad(parse_number(dt[ , CARD_NUMBER]), width = 4, side = 'left', pad = "0")]

#transform the transaction date from string to date format
dt[ , TRANS_DATE := as.Date(dt[ , TRANS_DATE], format = "%d/%m/%y")]

#dropping transaction codes because we are keeping the description
dt[ , c("TRANS_CAC_CODE_1", "TRANS_CAC_CODE_2", "TRANS_CAC_CODE_3") := NULL]

#dropping TRANS_VAT_DESC because there is not metadata and we cannot infer its meaning
dt[ , TRANS_VAT_DESC := NULL]

#summary
summary(dt)

#just 1 NA's in TRANS_DATE, so we remove it
dt <- dt[!is.na(TRANS_DATE)]

#original_gross_amt is a character column, we cast it as numeric
as.numeric(dt[ , ORIGINAL_GROSS_AMT])

#it warns as it has introduced NAs so something must be happening, we check the nans
dt[which(is.na(as.numeric(dt[ , ORIGINAL_GROSS_AMT])))]

#it is the thousands separator, we replace it and cast again as numeric
dt[ , ORIGINAL_GROSS_AMT := as.numeric(gsub(",", "", ORIGINAL_GROSS_AMT))]

##data exploration
#we explore our only numeric variable
hist(dt[ , ORIGINAL_GROSS_AMT])

#we see that we got outliers, so we limit our graph on the quantiles 1% and 99% (extreme values)
ext_q <- quantile(dt[ , ORIGINAL_GROSS_AMT], probs = c(0.01, 0.99))
hist(dt[between(ORIGINAL_GROSS_AMT, ext_q[1], ext_q[2]), ORIGINAL_GROSS_AMT])

#exploring number of distinct values in each column
#we declare a function for unique values
f <- function(x){
  length(unique(x))
}

#we apply the function to the margin 2 (columns)
apply(dt, MARGIN = 2, f)

#we see that 
#we have 370 different days with transactions
#18083 different monetary amounts in transactions
#6268 different merchants
#1028 different card numbers (assuming 1028 different clients)
#125 grouped type of business according to desc 1
#888 grouped type of business according to desc 2
#13 grouped type of business according to directorate

#we show the 20 most frequent desc 2
head(dt[ , .N, TRANS_CAC_DESC_2][order(N, decreasing = TRUE)], 20)

#we see that it is somehow related to institutions or schools but since we got no metadata and the are 88 categories we chose to drop it
dt[ , TRANS_CAC_DESC_2 := NULL]

#we show the 20 most frequent merchant names
head(dt[ , .N, MERCHANT_NAME][order(N, decreasing = TRUE)], 20)

#we see that amazon has more than 5 variations in its name, so we group it
dt[grepl('amazon', tolower(dt[ , MERCHANT_NAME])), MERCHANT_NAME := 'amazon']

#we do the same for other similar cases
dt[grepl('asda', tolower(dt[ , MERCHANT_NAME])), MERCHANT_NAME := 'asda']
dt[grepl('travelodge', tolower(dt[ , MERCHANT_NAME])), MERCHANT_NAME := 'travelodge']
dt[grepl('argos', tolower(dt[ , MERCHANT_NAME])), MERCHANT_NAME := 'argos']

#everything else below 500 trx is grouped in a bag
head(dt[ , .N, MERCHANT_NAME][order(N, decreasing = TRUE)], 20)
common.merchants <- c('amazon', 'travelodge', 'asda', 'argos', 'post office counter')
dt[!dt[ , MERCHANT_NAME] %in% common.merchants, MERCHANT_NAME := 'other']

#we show the 20 most frequent desc 1
head(dt[ , .N, TRANS_CAC_DESC_1][order(N, decreasing = TRUE)], 20)

#we keep the groups with more than 5% of total transactions and the rest is grouped in a bag
(gt5pct <- dt[ , .N, TRANS_CAC_DESC_1][order(N, decreasing = TRUE)][N > 0.05*nrow(dt), ][ , TRANS_CAC_DESC_1])
dt[!dt[ , TRANS_CAC_DESC_1] %in% gt5pct, TRANS_CAC_DESC_1 := 'other']

#we show the 10 most frequent directorate
dt[ , DIRECTORATE := toupper(DIRECTORATE)]
head(dt[ , .N, DIRECTORATE][order(N, decreasing = TRUE)], 10)

#we keep the groups with more than 5% of total transactions and the rest is grouped in a bag
(gt5pct <- dt[ , .N, DIRECTORATE][order(N, decreasing = TRUE)][N > 0.05*nrow(dt), ][ , DIRECTORATE])
dt[!dt[ , DIRECTORATE] %in% gt5pct, DIRECTORATE := 'other']
head(dt[ , .N, DIRECTORATE][order(N, decreasing = TRUE)], 10)
```

