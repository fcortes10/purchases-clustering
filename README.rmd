---
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  
***

> This report is just for assessment purposes

*by Fernando Cort√©s Tejada* | [linkedin](https://www.linkedin.com/in/fernando-cortes-tejada/) | [github](https://github.com/fcortes10)

# Purchase Card Transactions

We have a collection of purchase card transactions for the Birmingham City Council. This is a historical open source dataset and can be found in this [link](https://data.birmingham.gov.uk/dataset/purchase-card-transactions).

The aim of this analysis is to **discover profiles** or **unusual transactions**. In Data Science language this can be read as clustering and anomalies detection problems.

The card transactions data starts in April 2014 and ends in January 2018. We want to use the most recent complete year for the analysis so we chose the whole 2017 year. When looking at the raw data, December 2017 file has different type of data, so the file might be wrong. We switched to December 2016 to November 2017.

We chose to approach this problem with `R` instead of `python` because the most robust method for clustering when you have different data types, e.g. numerical, logical, categorical and ordinal, is Gower's distance and is not yet well implemented in a python package. The mathematical definition of this distance can be found [here](https://statisticaloddsandends.wordpress.com/2021/02/23/what-is-gowers-distance/).

***

## Index
1. [Data reading](#data-reading)  
2. [Data cleaning](#data-cleaning)  
3. [Data exploration](#data-exploration)
4. [Feature engineering](#feature-engineering)  
    + [Transaction level](#transaction-level)
    + [Client level](#client-level)  
5. [Clustering and anomalies detection](#clustering-and-anomalies-detection)  
    + [Number of clusters (K)](#number-of-clusters-k)  
    + [Clustering](#clustering)  
    + [Anomalies detection](#anomalies-detection)  
    + [New number of clusters (K)](#new-number-of-clusters-k)  
    + [New clustering](#new-clustering)  
6. [Interpretation and conclusions](#interpretation-and-conclusions)

***

## Let's get started

We begin by setting some global configurations and loading required packages. For installing the ones you don't have run `install.packages("package-name")` in the R console.

```{r config-packages, message=FALSE, cache=FALSE, results='hide'}
Sys.setlocale("LC_TIME", "C")

library(knitr)
library(data.table)
library(plyr)
library(readr)
library(stringr)
library(dplyr)
library(ggQC)
library(fastDummies)
library(cluster)
library(factoextra)
library(purrr)
library(mclust)
library(xgboost)
```

([back to index](#index))

***
### Data reading  

Then we read the data structure, get all files from the `data-csv` folder, also [here](https://github.com/fcortes10/purchases-clustering/tree/main/data-csv), standardize column names and append all datasets into a big one called `dt`.

```{r data reading, message=FALSE, cache=FALSE, results='hide'}
#reading structure from data
dt <- fread('data-csv/purchase-card-transactions-201612.csv', nrows = 0)

#getting file names
files <- list.files('data-csv', pattern = '..csv', full.names = TRUE)

#standardizing column names: no spaces and uppercase
column.names <- toupper(gsub(" ", "_", colnames(dt)))

#read all files and store them in a list
list.dt <- lapply(files, fread, select = colnames(dt), col.names = column.names)

#collapse all elements in one data.table
dt <- as.data.table(ldply(list.dt, data.frame))

#remove unnecessary/remaining environment variables and cleaning garbage in RAM
rm(list.dt, files, column.names)
gc()
```

Now we can have an overview of how the dataset looks like (just the 3 first rows)

```{r data_head, cache=FALSE}
head(dt, 3)
```

([back to index](#index))

***
### Data cleaning

We apply some treatments to data in order to make it more handleable. This includes, converting data types, changing formats, dropping columns, cleaning missing values, etc.

```{r data_cleaning_1}
#data cleaning
#extract just the numeric part from the card number as key (leaving it as string)
dt[ , CARD_NUMBER := str_pad(parse_number(dt[ , CARD_NUMBER]), width = 4, side = 'left', pad = "0")]

#transform the transaction date from string to date format
dt[ , TRANS_DATE := as.Date(dt[ , TRANS_DATE], format = "%d/%m/%y")]

#dropping transaction codes because we are keeping the description
dt[ , c("TRANS_CAC_CODE_1", "TRANS_CAC_CODE_2", "TRANS_CAC_CODE_3") := NULL]

#dropping TRANS_VAT_DESC because there is not metadata and we cannot infer its meaning
dt[ , TRANS_VAT_DESC := NULL]
```

We get a brief summary of the data to see any pattern or issue

```{r summary}
summary(dt)
```

The first variable we can see is `TRANS_DATE`, which has only one `NA`, so we remove it.

```{r drop_na}
dt <- dt[!is.na(TRANS_DATE)]
```

We also see that `ORIGINAL_GROSS_AMT` is a character column when it must be numeric. We cast it as numeric.

```{r cast_numeric_warning}
head(as.numeric(dt[ , ORIGINAL_GROSS_AMT]))
```

where we get a warning of induced `NAs`, so something must be happening. Checking the `NAs`

```{r na_check}
head(dt[which(is.na(as.numeric(dt[ , ORIGINAL_GROSS_AMT])))], 3)
```

we can see it is the thousands separator. So we replace the character `","` in the string and cast again.

```{r cast_numeric}
#it is the thousands separator, we replace it and cast again as numeric
dt[ , ORIGINAL_GROSS_AMT := as.numeric(gsub(",", "", ORIGINAL_GROSS_AMT))]
```

([back to index](#index))

***
### Data exploration

For the data exploration we don't want pretty charts yet, just see how the data looks. We start with out only numeric column `ORIGINAL_GROSS_AMT`. To explore a univariate numeric variable, the simplest way is plotting a histogram:

```{r histogram}
hist(dt[ , ORIGINAL_GROSS_AMT], main = "Histogram for gross amount", xlab = "Gross amount")
```

where we see that we got outliers that doesn't let us see clearly our data. So we limit our graph to be between the quantiles 5% and 95%
```{r quantiles_hist}
ext_q <- quantile(dt[ , ORIGINAL_GROSS_AMT], probs = c(0.05, 0.95))
hist(dt[between(ORIGINAL_GROSS_AMT, ext_q[1], ext_q[2]), ORIGINAL_GROSS_AMT], 
     main = "Histogram for gross amount (without tail values)", xlab = "Gross amount")
```

We can see a right skewed distribution, similar to a decaying exponential.

Now we will explore the categorical columns by checking the number of distinct values in each variable.

```{r unique_col_values}
#we declare a function for unique values
f <- function(x){
  length(unique(x))
}

#we apply the function to the margin 2 (columns)
apply(dt, MARGIN = 2, f)
```

We see that we have:  

* `370` distinct days with transactions
* `18,083` distinct monetary amounts in transactions
* `6,268` distinct merchant
* `1,028` distinct card numbers (and we can assume `1,028` distinct clients)
* `125` distinct type of business according to `DESC_1`
* `888` distinct type of business according to `DESC_2`
* `13` distinct type of business according to `DIRECTORATE`

We start with `TRANS_CAC_DESC_2` and show the 20 most frequent categories:

```{r desc_2}
head(dt[ , .N, TRANS_CAC_DESC_2][order(N, decreasing = TRUE)], 20)
```

where we can see that it is somehow related to institutions or schools but since we got no metadata and the are 88 categories we chose to drop it.

```{r drop_desc_2}
dt[ , TRANS_CAC_DESC_2 := NULL]
```

Then, we continue with the 20 most frequent `MERCHANT_NAME`

```{r merchant_name}
head(dt[ , .N, MERCHANT_NAME][order(N, decreasing = TRUE)], 20)
```

where we see that amazon has more than 5 variations in its name, so we group it

```{r amazon}
dt[grepl('amazon', tolower(dt[ , MERCHANT_NAME])), MERCHANT_NAME := 'amazon']
```

and we do the same for other similar cases.

```{r grouping_merchants}
dt[grepl('asda', tolower(dt[ , MERCHANT_NAME])), MERCHANT_NAME := 'asda']
dt[grepl('travelodge', tolower(dt[ , MERCHANT_NAME])), MERCHANT_NAME := 'travelodge']
dt[grepl('argos', tolower(dt[ , MERCHANT_NAME])), MERCHANT_NAME := 'argos']
```

We just keep the common merchants and everything else is grouped in a bag to reduce categories.

```{r merchants_bag}
head(dt[ , .N, MERCHANT_NAME][order(N, decreasing = TRUE)], 10)
common.merchants <- c('amazon', 'travelodge', 'asda', 'argos', 'post office counter')
dt[!dt[ , MERCHANT_NAME] %in% common.merchants, MERCHANT_NAME := 'other']
```

We do the same for `TRANS_CAC_DESC_1` and `DIRECTORATE` but we define a threshold of 5% not to put the category in a bag.

```{r casc_1}
#we show the 20 most frequent desc 1
head(dt[ , .N, TRANS_CAC_DESC_1][order(N, decreasing = TRUE)], 20)

#we keep the groups with more than 5% of total transactions and the rest is grouped in a bag
(gt5pct <- dt[ , .N, TRANS_CAC_DESC_1][order(N, decreasing = TRUE)][N > 0.05*nrow(dt), ][ , TRANS_CAC_DESC_1])
dt[!dt[ , TRANS_CAC_DESC_1] %in% gt5pct, TRANS_CAC_DESC_1 := 'other']
```

```{r directorate}
#we show the 10 most frequent directorate
dt[ , DIRECTORATE := toupper(DIRECTORATE)]
head(dt[ , .N, DIRECTORATE][order(N, decreasing = TRUE)], 10)

#we keep the groups with more than 5% of total transactions and the rest is grouped in a bag
(gt5pct <- dt[ , .N, DIRECTORATE][order(N, decreasing = TRUE)][N > 0.05*nrow(dt), ][ , DIRECTORATE])
dt[!dt[ , DIRECTORATE] %in% gt5pct, DIRECTORATE := 'other']
head(dt[ , .N, DIRECTORATE][order(N, decreasing = TRUE)], 10)
```

([back to index](#index))

***
### Feature engineering

We have divided the feature engineering in two groups: transaction level and client level.

#### Transaction level

We just have 6 columns and one is the key column (card_number) so just 5 features. Thus, we need to create more features in order to make clusters and find profiles.

Let's begin by extracting the day, weekday and month as variables.

```{r day_week_month}
#extract the day as a variable
dt[ , DAY := as.numeric(substr(x = TRANS_DATE, start = 9, stop = 10))]

#extract the weekday as a variable
dt[ , WEEKDAY := weekdays(dt[ , TRANS_DATE])]

#extract the months as a variable
dt[ , MONTH := as.numeric(substr(x = TRANS_DATE, start = 6, stop = 7))]
```

We create the `CHARGEBACK` feature, which tells us if the transaction amount is negative (a return) 

```{r chargeback}
#chargebacks
dt[ , CHARGEBACK := ifelse(ORIGINAL_GROSS_AMT < 0, 1, 0)]
```

and with that we change all amounts to positive.

```{r positive_amt}
#amounts to positive
dt[ , POSITIVE_AMT := ifelse(CHARGEBACK == 1, -1*ORIGINAL_GROSS_AMT, ORIGINAL_GROSS_AMT)]
```

We also create binary features that indicate us if the transaction amount is an outlier, an extreme value, is a tail value or is over the median.

```{r outlier_ext}
#outliers
iqr <- quantile(dt[ , POSITIVE_AMT], probs = c(0.25, 0.75))
dt[ , OUTLIER := ifelse(!between(POSITIVE_AMT, iqr[1]-1.5*QCrange(iqr), iqr[2]+1.5*QCrange(iqr)), 1, 0)]

#extreme values
dt[ , EXTREME_VALUE := ifelse(!between(POSITIVE_AMT, iqr[1]-3*QCrange(iqr), iqr[2]+3*QCrange(iqr)), 1, 0)]

#tail values
tails <- quantile(dt[ , POSITIVE_AMT], probs = c(0.025, 0.975))
dt[ , TAIL_VALUE := ifelse(!between(POSITIVE_AMT, tails[1], tails[2]), 1, 0)]

#over the median (otm)
median.value <- median(dt[ , POSITIVE_AMT])
dt[ , OTM := ifelse(POSITIVE_AMT > median.value, 1, 0)]
```

Now we create two more binary features related to transactions made around the payday or on weekends.

```{r payday_weekend}
#transactions around payday
paydays <- c(1, 2, 14, 15, 16, 28, 29, 30, 31)
dt[ , PAYDAY_TRX := ifelse(DAY %in% paydays, 1, 0)]

#transactions on weekends
weekend.days <- c('Saturday', 'Sunday')
dt[ , WEEKEND_TRX := ifelse(WEEKDAY %in% weekend.days, 1, 0)]
```

We see a summary with the new features.

```{r summary_trx}
summary(dt)
```

([back to index](#index))

#### Client level

Based on what we have just engineered at the transaction level, we begin to create features by grouping the information by client. 

First, we need to create dummies from the categorical variables.

```{r dummy_engineering}
#select columns to make dummies
cols.for.dummy <- c('MERCHANT_NAME', 'TRANS_CAC_DESC_1', 'DIRECTORATE', 'WEEKDAY')
dt.dummies <- dummy_cols(dt, select_columns = cols.for.dummy)
```


Then we start grouping by client and creating new features. These new features include averages, totals, sums, maximums, percentages, ratios and modes.

```{r group_eng}
#declare the statistical mode function (since R doesn't have one)
getmode <- function(x) {
  uniqv <- unique(x)
  uniqv[which.max(tabulate(match(x, uniqv)))]
}

#group by client
dt.grouped <- dt.dummies[ , .(NUM_TRX = .N, AVG_TRX = mean(POSITIVE_AMT), 
                MAX_TRX = max(POSITIVE_AMT), NUM_CHARGEBACKS = sum(CHARGEBACK),
                PCT_CHARGEBACKS = sum(CHARGEBACK)/.N, 
                AVG_AMT_CHARGEBACKS = mean(POSITIVE_AMT*CHARGEBACK),
                PCT_AMT_CHARGEBACKS = sum(POSITIVE_AMT*CHARGEBACK)/sum(POSITIVE_AMT),
                NUM_OUTLIER = sum(OUTLIER), PCT_OUTLIER = sum(OUTLIER)/.N,
                NUM_XTRM_VALUE = sum(EXTREME_VALUE), PCT_XTRM_VALUE = sum(EXTREME_VALUE)/.N,
                NUM_TAIL_VALUE = sum(TAIL_VALUE), PCT_TAIL_VALUE = sum(TAIL_VALUE)/.N,
                NUM_OTM = sum(OTM), PCT_OTM = sum(OTM)/.N, 
                NUM_PAYDAY_TRX = sum(PAYDAY_TRX), PCT_PAYDAY_TRX = sum(PAYDAY_TRX)/.N,
                NUM_WEEKEND_TRX = sum(WEEKEND_TRX), PCT_WEEKEND_TRX = sum(WEEKEND_TRX)/.N,
                MODE_MERCHANT = as.factor(getmode(MERCHANT_NAME)), 
                MODE_CAC_1 = as.factor(getmode(TRANS_CAC_DESC_1)),
                MODE_DIRECT = as.factor(getmode(DIRECTORATE)), 
                MODE_DAY = as.factor(getmode(DAY)), 
                MODE_MONTH = as.factor(getmode(MONTH))), 
                CARD_NUMBER]


```

Finally, we see a summary of the new client-level features.

```{r summary_group}
summary(dt.grouped)
```

([back to index](#index))

***
### Clustering and anomalies detection

#### Number of clusters (K)

([back to index](#index))

#### Clustering

([back to index](#index))

#### Anomalies detection

([back to index](#index))

#### New number of clusters (K)

([back to index](#index))

#### New clustering

([back to index](#index))

***
### Interpretation and conclusions

([back to index](#index))